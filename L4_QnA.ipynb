{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52824b89-532a-4e54-87e9-1410813cd39e",
      "metadata": {
        "id": "52824b89-532a-4e54-87e9-1410813cd39e"
      },
      "source": [
        "# LangChain: Q&A over Documents\n",
        "\n",
        "An example might be a tool that would allow you to query a product catalog for items of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c1f7b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10c1f7b9",
        "outputId": "99596333-d0f7-4d54-8881-7edb56559fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.337)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNw4M347WN6v",
        "outputId": "1171bee0-a610-4b44-d4ab-cdb630612132"
      },
      "id": "qNw4M347WN6v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXfsyQQeHtkS"
      },
      "id": "rXfsyQQeHtkS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DwhzQ2pXkxP",
        "outputId": "34204978-a8bc-4a10-fefc-c9aecd46b94d"
      },
      "id": "1DwhzQ2pXkxP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXKp8rFFtCQR",
        "outputId": "654d1772-f50e-4102-bc9a-ef88373fc78f"
      },
      "id": "XXKp8rFFtCQR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.3)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
      "metadata": {
        "tags": [],
        "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead635a0-42e2-46cc-a9f7-98419eceae6d",
      "metadata": {
        "id": "ead635a0-42e2-46cc-a9f7-98419eceae6d"
      },
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc533037-0b8c-4995-96a3-45b35fa13c18",
      "metadata": {
        "id": "cc533037-0b8c-4995-96a3-45b35fa13c18"
      },
      "outputs": [],
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo-1106\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-16k\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
      "metadata": {
        "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7249846e",
      "metadata": {
        "id": "7249846e"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"SurveyonVideoRec.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfaba30",
      "metadata": {
        "id": "5bfaba30"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b5ab657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b5ab657",
        "outputId": "b21ae226-7d23-4500-f74a-fbc4f8fc35df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docarray in /usr/local/lib/python3.10/dist-packages (0.39.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.23.5)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from docarray) (3.9.10)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.10.13)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (13.7.0)\n",
            "Requirement already satisfied: types-requests>=2.28.11.6 in /usr/local/lib/python3.10/dist-packages (from docarray) (2.31.0.10)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (4.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (2.16.1)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.10/dist-packages (from types-requests>=2.28.11.6->docarray) (2.0.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install docarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e200726",
      "metadata": {
        "id": "9e200726"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34562d81",
      "metadata": {
        "id": "34562d81"
      },
      "outputs": [],
      "source": [
        "query =\"Please list application of video action recognition in sports and summarize each one.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd0cc37",
      "metadata": {
        "id": "cfd0cc37"
      },
      "outputs": [],
      "source": [
        "response = index.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae21f1ff",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "ae21f1ff",
        "outputId": "40dd4f3b-0270-479f-f457-7a40de731792"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " Video action recognition in sports has a variety of applications, including training aids, analysis of athletes' performances and rehabilitation, multimedia intelligent devices with user-tailored digests, sports statistics analysis, and automatic understanding of sports. Training aids involve using the video action recognition to obtain the actions/events and then correlating the action sequences/combinations with winning strategies to guide the training of players or help with designing the game plan. Analysis of athletes' performances and rehabilitation involve using the video action recognition to analyze the performance of athletes and help with rehabilitation. Multimedia intelligent devices with user-tailored digests involve using the video action recognition to create user-tailored digests of sports videos. Sports statistics analysis involves using the video action recognition to collect and classify the actions/events in sports video, which is the basis to understand the sports. Automatic understanding of sports involves using the video action recognition to analyze the actions which occur in a video."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2534597e-4b0c-4563-a208-e2dd91064438",
      "metadata": {
        "id": "2534597e-4b0c-4563-a208-e2dd91064438"
      },
      "source": [
        "## Step By Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631396c6",
      "metadata": {
        "id": "631396c6"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"SurveyonVideoRec.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2164b5",
      "metadata": {
        "id": "6c2164b5"
      },
      "outputs": [],
      "source": [
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a977f44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a977f44",
        "outputId": "4dcc9751-4cb4-4e20-9219-1350574443e2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='14\\nFig. 6. A typical framework for group activity recognition (GAR). Compared\\nwith models for individual action recognition shown in Fig. 4, GAR models\\nnormally require player tracking, individual player feature extraction and\\ngroup feature combination, which is more complicated.\\nof the ball state – ﬂying, passed, possessed and out of play.\\nIn contrast, M. Ibrahim et al. [77] proposed a hierarchical\\ndeep model for GAR, where each player is detected ﬁrst and\\nthe dynamics of each player are modeled using a LSTM,\\nﬁnally, the a group-level LSTM is adopted to aggregate all\\nplayers’ dynamics and makes a prediction. The hierarchical\\ndeep model achieves 51.1% on HierVolleyball dataset and\\n81.9% on HierVolleyball-v2 .\\nT. Shu et al. [288] use a graph to model group activities,\\nproposing a Conﬁdence-Energy Recurrent Network (CERN).\\nSpeciﬁcally, CERN ﬁrst employs a tracker to obtain the\\ntrajectories of players and then constructs a graph, where each\\nnode represents an individual player position in a video frame\\nand each edge represents the relationship between two nodes.\\nTwo types of LSTMs are applied – node LSTM and edge\\nLSTM to compute deep features of graph nodes and edges.\\nCERN achieves 83.6% on HierVolleyball-v2 .\\nIn contrast, T. Bagautdinov et al. [289] proposed an end-\\nto-end approach for GAR, where player detection and ac-\\ntion recognition adopts a shared fully-connected CNN. The\\ndetection branch applies Markov Random Field (MRF) to\\nreﬁne the predicted player positions and the classiﬁcation\\nbranch uses a matching Recurrent Neural Network (RNN) to\\npredict individual’s action and their group activity. Without\\nextra tracking models, the proposed model takes less time for\\ntraining and inference. In terms of the performance, it obtains\\n87.1% accuracy on HierVolleyball-v2 .\\nSimilarly, RCRG [23] extend the two-stage framework in\\n[288], [289] via introducing a hierarchical relational network,\\nwhich is similar to graph neural networks, i.e., the new repre-\\nsentation of a node is obtained by aggregating the information\\nof its neighbors.\\nStageNet [29] is composed of 4 stages: player detection,\\nsemantic graph construction, temporal information integrationTABLE VI\\nDEEP LEARNING MODEL FOR GROUP ACTIVITY RECOGNITION IN SPORTS .\\nModel Venue HierV olleyball-v2\\nM. Ibrahim et al. [77] CVPR-2016 81.9\\nCERN [288] CVPR-2017 83.6\\nT. Bagautdinov et al. [289] CVPR-2017 87.1\\nRCRG [23] ECCV-2018 89.5\\nStageNet [29] TCSVT 89.3\\nPOGARS [82] Arxiv 93.9\\nAnchor-Transformer [290] CVPR-2020 94.4\\nDIN [291] CVPR-2021 93.1\\nPose3D [147] CVPR-2022 91.3\\nand spatial-temporal attention. Player detection and seman-\\ntic graph construction are similar to RCRG [23], i.e., each\\nnode of the graph represent a player position and the edges\\nrepresent the relationships determined by the spatial distance\\nand temporal correlations among players. In terms of temporal\\ninformation integration, structural RNNs – node RNN and\\nedge RNN are applied and ﬁnally the aggregated information\\nis fed into spatial-temporal module. Using spatial-temporal\\nattention makes StageNet more explainable.\\nRecently, the poses of players are introduced into GAR.\\nH. Thilakarathne et al. [82] propose a Pose Only Group\\nActivity Recognition System (POGARS), which consists of two\\nkey modules – player tracking and pose estimation and each\\nplayer is represented by 16 2D keypoints. After that, POGARS\\nstacks multiple temporal and spatial convolutioanl layers to\\nobtain high-level player representations. In addition, POGARS\\ninvestigates different person-level fusion approaches, including\\nearly fusion and late fusion. Finally, POGARS achieves 93.2%\\naccuracy on HierVolleyball-v2 and the performance can be\\nfurther improved to 93.9% by using both player poses and\\nball tracklets. While Pose3D [147] adopts skeleton heatmaps\\ninstead of the 2D coordinates and the feature extraction model\\nis a 3D CNN, achieving 91.3% accuracy.\\nH. Yuan et al. [291] introduces dynamic relation (DR) and\\ndynamic walk (DW) into GAR models, proposing a Dynamic\\nInference Network (DIN), where the detected players are\\nconstructed into a spatial-temporal graph and then DR is used\\nto predict the relationships among players and DW is used\\nto predict the dynamic walk offset to allow global interaction\\nover the entire spatial-temporal graph. Using DR and DW,\\nDIN obtains 93.1% on HierVolleyball-v2 .\\nK. Gavrilyuk et al. [290] propose a transformer based\\nmodel – Anchor-Transformer, where the representations of\\ndifferent players are fused via a transformer instead of a\\nLSTM. Similarly, Anchor-Transformer ﬁrst employs a player\\ndetection model to obtain the individuals and then fuses the\\nindividual embeddings using a transformer for classiﬁcation.\\nIt achieves 94.4% on HierVolleyball-v2 using both pose and\\noptical ﬂow.\\nApart from volleyball, GAR in football is also investigated.\\nT. Tsunoda et al. [43] propose a hierarchical LSTM model\\nto recognition football team activities, which is similar to the\\nmodel in [77], but the videos in football dataset is captured\\nby multiple synchronized cameras.\\nAlso, we present the performances of different models in\\nTable VI. Note that most models conduct experiments on', metadata={'source': 'SurveyonVideoRec.pdf', 'page': 13})"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "docs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e875693a",
      "metadata": {
        "id": "e875693a"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779bec75",
      "metadata": {
        "id": "779bec75"
      },
      "outputs": [],
      "source": [
        "embed = embeddings.embed_query(\"Summarize with bullet points GROUP/TEAM ACTIVITY RECOGNITION part\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "699aaaf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699aaaf9",
        "outputId": "fa0d7ea0-c897-4c03-d821-eb47734ab1c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        }
      ],
      "source": [
        "print(len(embed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d00d346",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d00d346",
        "outputId": "ceed1303-76a7-4dc6-cf7e-df007d7ab1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.028930572816191998, 0.00599422352871351, 0.016225684447245693, -0.017467515557173744, -0.017835970982375368]\n"
          ]
        }
      ],
      "source": [
        "print(embed[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ad0bb0",
      "metadata": {
        "id": "27ad0bb0"
      },
      "outputs": [],
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs,\n",
        "    embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0329bfd5",
      "metadata": {
        "id": "0329bfd5"
      },
      "outputs": [],
      "source": [
        "query = \"Summarize with bullet points GROUP/TEAM ACTIVITY RECOGNITION part\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7909c6b7",
      "metadata": {
        "id": "7909c6b7"
      },
      "outputs": [],
      "source": [
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43321853",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43321853",
        "outputId": "abad63c2-9b53-4731-8f40-76b4edbb4497"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6eba90b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eba90b5",
        "outputId": "5817129d-314f-4132-9172-30bb76714c14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='13\\nTABLE V\\nCURRENT STATE OF INDIVIDUAL SPORTS VIDEO ACTION RECOGNITION .\\nHERE WE ONLY LIST THE PERFORMANCE ON THE SPORTS -RELATED\\nDATASETS NOT IN TABLE IV.\\nSports Dataset Model Year Performance\\nTennisACASV A [189] HOG3D+CNN [216] 2020 93.78\\nTHETIS [190] Lightweight 3D [280] 2022 90.9\\nTenniSet [195] Two-stream [195] 2017 81.0\\nTable tennisTTStroke-21 [130] Two-stream [25] 2020 91.4\\nSPIN [200] Multi-stream [200] 2019 72.8\\nStroke Recognition [138] TCN [281] 2021 99.37\\nBadminton Badminton Olympic [159] TCN [281] 2018 71.49\\nBasketballNCAA [58] CNN+LSTM [58] 2016 51.6\\nFineBasketball [64] TSN-Two-Stream [238] 2020 29.78\\nNPUBasketball [67] Skeleton-based [67] 2020 80.9\\nFootball SoccerNet [24] 3D [24] 2018 65.2\\nOthersHockey Fight [85] Two-stream [282] 2017 97.0\\nGolfDB [201] CNN+LSTM [201] 2019 79.2\\nFenceNet [211] TCN [281] 2022 87.6\\noutput a response, where the queries are texts like the\\nnumber of flips for diving and the responses are the\\ncorresponding attributes, such as a number or a label. The\\ntransformer-based decoder models the relevance among visual\\nfeatures, queries and responses. In terms of ﬁne-grained action\\nrecognition, TQN requires to pre-deﬁned action labels and\\neach label has a set of attributes for classiﬁcation, hence, we\\ncan classify the actions based on the responses. Compared\\nwith its 3D counterparts, TNQ shows its superiority, achieving\\n89.6% on FineGym and 81.8% on Diving48 .\\nNote that videos are composed of not only frames but\\nalso audios, and there a family of models that adopt multiple\\nmodalities. Similar to two-stream models, multimodal models\\nconsists of several branches. One recent work is AudioSlow-\\nFast [284] proposed by F. Xiao et al. , where acoustic informa-\\ntion is introduced into the original SlowFast [12] model using\\nan audio branch, hence, AudioSlwoFast has 3 branches – slow,\\nfast and audio. While Y . Bian et al. [285] propose an ensemble\\nmodel that adopts video frames, optical ﬂow and audio. In\\nour developed toolbox2, we also adopt acoustic information\\nto classify football actions, where there are 8 categories, such\\nas red card, corner and free kick. Using multiple modalities\\nis able to improve the capacity of deep models and the\\nredundant information could make the model more robust,\\nhowever, it is difﬁcult to combine different modalities and\\ntraining multimodal models is non-trivial [261]. In addition,\\nusing more branches leads to a large models, so overﬁtting\\ncan easily occur.\\nIn Table V, we present current state of action recognition in\\ndifferent types of sports. We can see that 3D and two-stream\\nmodels are relatively popular and the recent advanced models\\nlike MoViNet [251] are rarely used in sports. One possible\\nreason is that some sports-related datasets lack challenges and\\ntwo-stream models can achieve high accuracy, for example,\\n91.4% on TTStroke-21 [130]. While some other datasets like\\nNCAA [58] and FineBasketball [64] are still challenging,\\nrequiring more advanced models.\\nIV. G ROUP /TEAM ACTIVITY RECOGNITION\\nGroup/team activity recognition is one branch of human\\nactivity recognition problem which targets the collective be-\\n2https://github.com/PaddlePaddle/PaddleVideo\\nGAR:  one group action is recognized as \"T riple-block\".\\nMAR:  three actions are recognized as \"Block\" in parallel.IAR:  an action is\\nrecognized as\\n\"Block\".Fig. 5. An example of individual, group, and multi-player activity recognition\\nin a frame of volleyball competition video.\\nhavior of a group of people, resulted from the individual\\nactions of the persons and their interactions. It is a basic task\\nfor automatic human behavior analysis in many areas, such\\nassports , health care and surveillance. Note that, although\\ngroup/team activity is conceptually an activity performed by\\nmutiple people or objects, the group/team activity recognition\\n(GAR) is quite different from another common task – the\\nmulti-player activity recognition (MAR) [286]. The former\\nis the process of recognizing activities of multiple players,\\nwhere a single group activity is a function of the action of\\neach and every player within the group [54]. The activity\\nof group can be observed as spontaneous emergent action,\\nconducted by the activities and interactions of individuals\\nwithin it. While the latter is the recognition of separate actions\\nof multiple players in parallel, where two or more players\\nparticipates. Figure 5 shows the differences among individual\\naction recognition (IAR), GAR, and MAR respectively. The\\nGAR example (yellow box) shows that where without knowl-\\nedge of all of the players in the opposite of the net, it is\\nimprobable that the algorithm will infer the accurate actions\\n(e.g., if one of the player does not participate the blocking,\\nthe activity is “double-block” indeed). Only observing all\\nsubjects provides enough evidence for the correct recognition.\\nTherefore, GAR is more challenging than individual action\\nrecognition, requiring to combine multiple computer vision\\ntechniques, such as player detection, pose estimation and ball\\ntracking. Fig. 6 presents a typical framework for GAR.\\nAn early work on group activity recognition is proposed\\nby W. Choi et al. in 2009 [287]. The proposed framework\\nis composed of people detection, tracking, pose estimation,\\nspatial-temporal local descriptor and classiﬁer, where hand-\\ncrafted features – HOG is adopted. Though W. Choi et al. only\\ntest the proposed framework on their own dataset for GAR, it\\ninspires the following approaches.\\nA. Maksai et al. [3] propose a approach to model the\\ninteraction between players and ball for GAR. The proposed\\napproach employs graphical models to track the ball and detect\\nplayers, resulting in a player graph and a ball graph. In the\\nplayer graph, each node represents a play location. With mas-\\nsage passing over the two graphs, the proposed approach can\\nmodel the interaction between the ball and players. However,\\nthe main purpose of this work is ball tracking and the settings\\nof GAR lack challenge, for example, there are only 4 classes', metadata={'source': 'SurveyonVideoRec.pdf', 'page': 12})"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c3596e",
      "metadata": {
        "id": "c0c3596e"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0625f5e8",
      "metadata": {
        "id": "0625f5e8"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a573f58a",
      "metadata": {
        "id": "a573f58a"
      },
      "outputs": [],
      "source": [
        "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14682d95",
      "metadata": {
        "id": "14682d95"
      },
      "outputs": [],
      "source": [
        "response = llm.call_as_llm(f\"{qdocs} Question: Please list application of video action recognition in sports and summarize each one.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bba545b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "8bba545b",
        "outputId": "62215603-572e-4a75-8e77-a0d10420da64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The applications of video action recognition in sports include:\n\n1. Training Aids: Video action recognition can provide valuable information for sports coaches and players to analyze and extract useful tactics. It can help guide training and improve performance.\n\n2. Game Assistance (Video Judge): Video-based game judges can use action recognition to evaluate the execution of sports performances and make accurate judgments. This can be used in sports like diving, skating, and referee training.\n\n3. Video Highlights: Action recognition can be used to segment and summarize video highlights in sports. It can identify key actions and poses that are commonly featured in highlight clips.\n\n4. Automatic Sports News Generation (ASNG): Action recognition can automatically generate statistical numbers and textual descriptions for sports news. This can save time and reduce workload in news generation.\n\n5. General Research Purposes: Action recognition in sports videos is a popular research topic. It can be used for analysis of athletes' performances, rehabilitation, multimedia intelligent devices, and sports statistics analysis.\n\nEach application has its own specific goals and challenges, and action recognition plays a crucial role in achieving those goals."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The optimized varsion of the task above**"
      ],
      "metadata": {
        "id": "ncVKQ0Ew9tIX"
      },
      "id": "ncVKQ0Ew9tIX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c94d22",
      "metadata": {
        "id": "32c94d22"
      },
      "outputs": [],
      "source": [
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4769316",
      "metadata": {
        "id": "e4769316"
      },
      "outputs": [],
      "source": [
        "query =  \"Please list application of video action recognition in sports and summarize each one.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc3c2f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fc3c2f3",
        "outputId": "64d3f119-2140-4170-ac18-1b85d5e7bb8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = qa_stuff.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba1a5db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "fba1a5db",
        "outputId": "be6d654e-c204-4627-a493-0143187b6da0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "1. Training Aids: Video action recognition can provide valuable information for sports coaches and players to analyze and extract useful tactics. Coaches and players can use the recognized actions/events to guide training and design game plans. For example, recognizing hockey players' poses and actions can help coaches assess player performance. AI coach systems can also use action recognition to support complex visual information extraction and summarization.\n\n2. Game Assistance (Video Judge): Video-based game judges are widely used in modern sports video analysis systems. Action recognition is an essential module in these systems. For example, a virtual referring network can evaluate the execution of a diving performance based on visual clues and body actions. Action recognition can also be used to improve the quality assessment of actions in diving. Additionally, sports referee training systems can recognize whether a trainee makes proper judging signals using deep belief networks.\n\n3. Video Highlights: Action recognition plays a crucial role in segmenting and summarizing video highlights in sports. By accurately recognizing actions, automatic highlight detection methods can locate and stitch target poses in skating videos. Recognizing 3D jump actions in figure skating can help recover poor-visualizing actions. Action recognition can also be used to improve the overall quality of video highlights by maximizing diversity and reducing recognition errors.\n\n4. Automatic Sports News Generation (ASNG): Video action recognition can be used to automatically generate sports news. Instead of manually providing statistical numbers, action recognition techniques can automatically generate these numbers from videos, saving time and reducing workload. Additionally, visual captioning techniques can be used to directly generate textual descriptions from videos. Recognizing player actions is essential for improving the naturalness, fluency, and accuracy of the generated texts.\n\n5. General Research Purposes: Action recognition in sports is a popular research topic. Sports videos provide a significant portion of the target video categories, and many research contributions benchmark on sports datasets. Action recognition techniques can efficiently collect and classify actions/events in sports videos, supporting sports statistics analysis and automatic understanding of sports.\n\nOverall, video action recognition in sports has various applications, including training aids, game assistance, video highlights, automatic sports news generation, and general research purposes. These applications contribute to improving coaching and training, enhancing game analysis, generating engaging sports content, and advancing research in sports analytics."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500ec062",
      "metadata": {
        "id": "500ec062"
      },
      "outputs": [],
      "source": [
        "response = index.query(query, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cffb19f",
      "metadata": {
        "id": "2cffb19f"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=embeddings,\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b325340f-26b4-4c7e-81da-da4b895ae058",
      "metadata": {
        "id": "b325340f-26b4-4c7e-81da-da4b895ae058"
      },
      "source": [
        "Reminder: Download your notebook to you local computer to save your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d590b337",
      "metadata": {
        "id": "d590b337"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2cb587c",
      "metadata": {
        "id": "b2cb587c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec249f1",
      "metadata": {
        "id": "1ec249f1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d64f166",
      "metadata": {
        "id": "9d64f166"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21322e7e",
      "metadata": {
        "id": "21322e7e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}