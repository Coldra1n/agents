{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "52824b89-532a-4e54-87e9-1410813cd39e",
      "metadata": {
        "id": "52824b89-532a-4e54-87e9-1410813cd39e"
      },
      "source": [
        "# LangChain: Evaluation\n",
        "\n",
        "## Outline:\n",
        "\n",
        "* Example generation\n",
        "* Manual evaluation (and debuging)\n",
        "* LLM-assisted evaluation\n",
        "* LangChain evaluation platform"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "!pip install pypdf\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install --upgrade langchain\n",
        "!pip install docarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwCH9NrPHuty",
        "outputId": "080415e1-eeaf-4502-8859-ec696ae1e343"
      },
      "id": "NwCH9NrPHuty",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.3)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.337)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.65)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: docarray in /usr/local/lib/python3.10/dist-packages (0.39.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.23.5)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from docarray) (3.9.10)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.10.13)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (13.7.0)\n",
            "Requirement already satisfied: types-requests>=2.28.11.6 in /usr/local/lib/python3.10/dist-packages (from docarray) (2.31.0.10)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (4.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (2.16.1)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.10/dist-packages (from types-requests>=2.28.11.6->docarray) (2.0.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "b7ed03ed-1322-49e3-b2a2-33e94fb592ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945d8abb-ba55-40a4-a3c5-6ad0dab73e3e",
      "metadata": {
        "id": "945d8abb-ba55-40a4-a3c5-6ad0dab73e3e"
      },
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "24ff81cd-dce0-4344-8d45-4a98fd3a87c9",
      "metadata": {
        "height": 234,
        "id": "24ff81cd-dce0-4344-8d45-4a98fd3a87c9"
      },
      "outputs": [],
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo-16k\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-16k\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28008949",
      "metadata": {
        "id": "28008949"
      },
      "source": [
        "## Create our QandA application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b",
      "metadata": {
        "height": 98,
        "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ec1106d",
      "metadata": {
        "height": 64,
        "id": "9ec1106d"
      },
      "outputs": [],
      "source": [
        "#file = 'SurveyonVideoRec.pdf'\n",
        "loader = PyPDFLoader(\"SurveyonVideoRec.pdf\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b31c218f",
      "metadata": {
        "height": 64,
        "id": "b31c218f"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2006054",
      "metadata": {
        "height": 183,
        "id": "a2006054"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.0, model=llm_model)\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True,\n",
        "    chain_type_kwargs = {\n",
        "        \"document_separator\": \"<<<<>>>>>\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791ebd73",
      "metadata": {
        "id": "791ebd73"
      },
      "source": [
        "### Coming up with test datapoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fb04a0f9",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb04a0f9",
        "outputId": "bdc21e31-64b7-402f-fef1-730259675f5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='24\\n[261] W. Wang, D. Tran, and M. Feiszli, “What makes training multi-\\nmodal classiﬁcation networks hard?” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2020, pp.\\n12 695–12 705.\\n[262] S. Yan, Y . Xiong, and D. Lin, “Spatial temporal graph convolutional\\nnetworks for skeleton-based action recognition,” in Thirty-second AAAI\\nconference on artiﬁcial intelligence , 2018.\\n[263] L. Shi, Y . Zhang, J. Cheng, and H. Lu, “Skeleton-based action\\nrecognition with multi-stream adaptive graph convolutional networks,”\\nIEEE Transactions on Image Processing , vol. 29, pp. 9532–9545, 2020.\\n[264] Y .-F. Song, Z. Zhang, C. Shan, and L. Wang, “Stronger, faster and more\\nexplainable: A graph convolutional baseline for skeleton-based action\\nrecognition,” in proceedings of the 28th ACM international conference\\non multimedia , 2020, pp. 1625–1633.\\n[265] Y . Chen, Z. Zhang, C. Yuan, B. Li, Y . Deng, and W. Hu, “Channel-\\nwise topology reﬁnement graph convolution for skeleton-based action\\nrecognition,” in Proceedings of the IEEE/CVF International Conference\\non Computer Vision , 2021, pp. 13 359–13 368.\\n[266] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,\\n“An image is worth 16x16 words: Transformers for image recognition\\nat scale,” arXiv preprint arXiv:2010.11929 , 2020.\\n[267] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\\ncomputation , vol. 9, no. 8, pp. 1735–1780, 1997.\\n[268] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[269] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:\\nA large-scale hierarchical image database,” in 2009 IEEE conference\\non computer vision and pattern recognition . Ieee, 2009, pp. 248–255.\\n[270] S. Ji, W. Xu, M. Yang, and K. Yu, “3d convolutional neural networks\\nfor human action recognition,” IEEE transactions on pattern analysis\\nand machine intelligence , vol. 35, no. 1, pp. 221–231, 2012.\\n[271] S. Pouyanfar, S.-C. Chen, and M.-L. Shyu, “An efﬁcient deep residual-\\ninception network for multimedia classiﬁcation,” in 2017 IEEE Inter-\\nnational Conference on Multimedia and Expo (ICME) . IEEE, 2017,\\npp. 373–378.\\n[272] G. Bender, H. Liu, B. Chen, G. Chu, S. Cheng, P.-J. Kindermans,\\nand Q. V . Le, “Can weight sharing outperform random architecture\\nsearch? an investigation with tunas,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2020, pp.\\n14 323–14 332.\\n[273] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in\\nProceedings of the IEEE international conference on computer vision ,\\n2017, pp. 2961–2969.\\n[274] H. Bao, L. Dong, and F. Wei, “Beit: Bert pre-training of image\\ntransformers,” arXiv preprint arXiv:2106.08254 , 2021.\\n[275] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen,\\nand I. Sutskever, “Zero-shot text-to-image generation,” in International\\nConference on Machine Learning . PMLR, 2021, pp. 8821–8831.\\n[276] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski,\\nand A. Joulin, “Emerging properties in self-supervised vision trans-\\nformers,” in Proceedings of the IEEE/CVF International Conference\\non Computer Vision , 2021, pp. 9650–9660.\\n[277] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\\nconvolutional networks,” arXiv preprint arXiv:1609.02907 , 2016.\\n[278] C. Si, W. Chen, W. Wang, L. Wang, and T. Tan, “An attention\\nenhanced graph convolutional lstm network for skeleton-based action\\nrecognition,” in proceedings of the IEEE/CVF conference on computer\\nvision and pattern recognition , 2019, pp. 1227–1236.\\n[279] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,” in International\\nconference on machine learning . PMLR, 2015, pp. 448–456.\\n[280] T. E. Rasmussen, L. H. Clemmensen, and A. Baum, “Compressing cnn\\nkernels for videos using tucker decompositions: Towards lightweight\\ncnn applications,” arXiv preprint arXiv:2203.07033 , 2022.\\n[281] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal\\nconvolutional networks for action segmentation and detection,” in\\nproceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , 2017, pp. 156–165.\\n[282] P. Zhou, Q. Ding, H. Luo, and X. Hou, “Violent interaction detection\\nin video based on deep learning,” in Journal of physics: conference\\nseries , vol. 844, no. 1. IOP Publishing, 2017, p. 012044.\\n[283] C. Zhang, A. Gupta, and A. Zisserman, “Temporal query networks for\\nﬁne-grained video understanding,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2021, pp.\\n4486–4496.[284] F. Xiao, Y . J. Lee, K. Grauman, J. Malik, and C. Feichtenhofer,\\n“Audiovisual slowfast networks for video recognition,” arXiv preprint\\narXiv:2001.08740 , 2020.\\n[285] Y . Bian, C. Gan, X. Liu, F. Li, X. Long, Y . Li, H. Qi, J. Zhou,\\nS. Wen, and Y . Lin, “Revisiting the effectiveness of off-the-shelf\\ntemporal modeling approaches for large-scale video classiﬁcation,”\\narXiv preprint arXiv:1708.03805 , 2017.\\n[286] D. Gordon, “Group activity recognition using wearable sensing de-\\nvices,” 2014.\\n[287] W. Choi, K. Shahid, and S. Savarese, “What are they doing?: Collective\\nactivity classiﬁcation using spatio-temporal relationship among peo-\\nple,” in 2009 IEEE 12th international conference on computer vision\\nworkshops, ICCV Workshops . IEEE, 2009, pp. 1282–1289.\\n[288] T. Shu, S. Todorovic, and S.-C. Zhu, “Cern: conﬁdence-energy re-\\ncurrent network for group activity recognition,” in Proceedings of the\\nIEEE conference on computer vision and pattern recognition , 2017,\\npp. 5523–5531.\\n[289] T. Bagautdinov, A. Alahi, F. Fleuret, P. Fua, and S. Savarese, “Social\\nscene understanding: End-to-end multi-person action localization and\\ncollective activity recognition,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition , 2017, pp. 4315–4324.\\n[290] K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, “Actor-\\ntransformers for group activity recognition,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2020, pp. 839–848.\\n[291] H. Yuan, D. Ni, and M. Wang, “Spatio-temporal dynamic inference net-\\nwork for group activity recognition,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , 2021, pp. 7476–7485.\\n[292] J. Wang, K. Qiu, H. Peng, J. Fu, and J. Zhu, “Ai coach: Deep\\nhuman pose estimation and analysis for personalized athletic training\\nassistance,” in Proceedings of the 27th ACM International Conference\\non Multimedia , 2019, pp. 374–382.\\n[293] T.-Y . Pan, W.-L. Tsai, C.-Y . Chang, C.-W. Yeh, and M.-C. Hu, “A\\nhierarchical hand gesture recognition framework for sports referee\\ntraining-based emg and accelerometer sensors,” IEEE Transactions on\\nCybernetics , 2020.\\n[294] N. Shroff, P. Turaga, and R. Chellappa, “Video précis: Highlighting\\ndiverse aspects of videos,” IEEE Transactions on Multimedia , vol. 12,\\nno. 8, pp. 853–868, 2010.\\n[295] J. Kanerva, S. Rönnqvist, R. Kekki, T. Salakoski, and F. Ginter,\\n“Template-free data-to-text generation of ﬁnnish sports news,” arXiv\\npreprint arXiv:1910.01863 , 2019.\\n[296] J. Gong, W. Ren, and P. Zhang, “An automatic generation method\\nof sports news based on knowledge rules,” in 2017 IEEE/ACIS 16th\\nInternational Conference on Computer and Information Science (ICIS) .\\nIEEE, 2017, pp. 499–502.\\n[297] Q. Wang, J. Wang, A. B. Chan, S. Huang, H. Xiong, X. Li, and D. Dou,\\n“Neighbours matter: Image captioning with similar images,” in 31st\\nBritish Machine Vision Virtual Conference (BMVC 2020) . British\\nMachine Vision Association, BMV A, 2020.\\n[298] Q. Wang, J. Wan, and A. B. Chan, “On diversity in image captioning:\\nMetrics and methods,” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence , 2020.\\n[299] J. Wang, W. Xu, Q. Wang, and A. B. Chan, “On distinctive image\\ncaptioning via comparing and reweighting,” IEEE Transactions on\\nPattern Analysis and Machine Intelligence , 2022.\\n[300] S. Chen and Y .-G. Jiang, “Motion guided region message passing\\nfor video captioning,” in Proceedings of the IEEE/CVF International\\nConference on Computer Vision , 2021, pp. 1543–1552.\\n[301] Z. Zhang, Z. Qi, C. Yuan, Y . Shan, B. Li, Y . Deng, and W. Hu,\\n“Open-book video captioning with retrieve-copy-generate network,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition , 2021, pp. 9837–9846.\\n[302] M. Ramanathan, W.-Y . Yau, and E. K. Teoh, “Human action recognition\\nwith video data: research and evaluation challenges,” IEEE Transac-\\ntions on Human-Machine Systems , vol. 44, no. 5, pp. 650–663, 2014.\\n[303] S. Herath, M. Harandi, and F. Porikli, “Going deeper into action\\nrecognition: A survey,” Image and vision computing , vol. 60, pp. 4–21,\\n2017.\\n[304] P. Pareek and A. Thakkar, “A survey on video-based human action\\nrecognition: recent updates, datasets, challenges, and applications,”\\nArtiﬁcial Intelligence Review , vol. 54, no. 3, pp. 2259–2322, 2021.\\n[305] Y . Kong and Y . Fu, “Human action recognition and prediction: A\\nsurvey,” International Journal of Computer Vision , pp. 1–36, 2022.\\n[306] D. Wu, N. Sharma, and M. Blumenstein, “Recent advances in video-\\nbased human action recognition using deep learning: A review,” in 2017', metadata={'source': 'SurveyonVideoRec.pdf', 'page': 23})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data[23]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fe4a88c2",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe4a88c2",
        "outputId": "6474d6df-d4e3-4cdb-da88-974a9b87cf67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d548aef",
      "metadata": {
        "id": "8d548aef"
      },
      "source": [
        "### Hard-coded examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c2d59bf2",
      "metadata": {
        "height": 217,
        "id": "c2d59bf2"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"What are five application of video action recognition in sports?Dislay only a list of categories without any additional comments\",\n",
        "        \"answer\": \"Training Aids, Game Assistance (Video Judge),  Video Highlights,  Automatic Sports News Generation (ASNG),  General Research Purposes\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are key challenges wof applying those action recognition baselines on sports videos in practical?Dislay only a list of categories without any additional comments\",\n",
        "        \"answer\": \"Data Collection and Annotation; Camera Motion, Cut and Occlusion; Long-tailed Distribution and Imbalanced Data; Dense and Fast-moving Actions ;  Transfer, Few-shot and Zero-shot Learning;  Multi-camera and Multi-view Action Recognition \"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ce3e4f",
      "metadata": {
        "id": "c7ce3e4f"
      },
      "source": [
        "### LLM-Generated examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d44f8376",
      "metadata": {
        "height": 47,
        "id": "d44f8376"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAGenerateChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "34e87816",
      "metadata": {
        "height": 30,
        "id": "34e87816"
      },
      "outputs": [],
      "source": [
        "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "31fb5bd0-fc2a-478c-9a09-6fe7e3307241",
      "metadata": {
        "height": 30,
        "id": "31fb5bd0-fc2a-478c-9a09-6fe7e3307241"
      },
      "outputs": [],
      "source": [
        "# the warning below can be safely ignored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "62abae09",
      "metadata": {
        "height": 64,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62abae09",
        "outputId": "6bd595f1-ad6f-4233-dde0-ab0005729389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:349: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "new_examples = example_gen_chain.apply_and_parse(\n",
        "    [{\"doc\": document} for document in data[:5]]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F5RAMCTLFxT",
        "outputId": "eec8c298-b00e-4356-db7d-6db8d1e18162"
      },
      "id": "5F5RAMCTLFxT",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'qa_pairs': {'query': 'What is the focus of the survey presented in the document?',\n",
              "  'answer': 'The focus of the survey presented in the document is video action recognition in sports analytics.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reformatted_examples = [\n",
        "    {\"query\": qa_pair['qa_pairs']['query'], \"answer\": qa_pair['qa_pairs']['answer']}\n",
        "    for qa_pair in new_examples\n",
        "]"
      ],
      "metadata": {
        "id": "5l-DoJ_EKfC9"
      },
      "id": "5l-DoJ_EKfC9",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reformatted_examples[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9_rb-LQLInf",
        "outputId": "aeb74946-ecf5-4b57-f509-7fba27d83017"
      },
      "id": "l9_rb-LQLInf",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is the focus of the survey presented in the document?',\n",
              " 'answer': 'The focus of the survey presented in the document is video action recognition in sports analytics.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples += reformatted_examples"
      ],
      "metadata": {
        "id": "Wb3S8IqsKhWQ"
      },
      "id": "Wb3S8IqsKhWQ",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra1c4tlcMiFZ",
        "outputId": "f19ae7fd-21a0-456e-8a60-739a0e3e7557"
      },
      "id": "ra1c4tlcMiFZ",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'query': 'What are five application of video action recognition in sports?Dislay only a list of categories without any additional comments',\n",
              "  'answer': 'Training Aids, Game Assistance (Video Judge),  Video Highlights,  Automatic Sports News Generation (ASNG),  General Research Purposes'},\n",
              " {'query': 'What are key challenges wof applying those action recognition baselines on sports videos in practical?Dislay only a list of categories without any additional comments',\n",
              "  'answer': 'Data Collection and Annotation; Camera Motion, Cut and Occlusion; Long-tailed Distribution and Imbalanced Data; Dense and Fast-moving Actions ;  Transfer, Few-shot and Zero-shot Learning;  Multi-camera and Multi-view Action Recognition '},\n",
              " {'query': 'What is the focus of the survey presented in the document?',\n",
              "  'answer': 'The focus of the survey presented in the document is video action recognition in sports analytics.'},\n",
              " {'query': 'According to the document, what are some examples of individual sports mentioned?',\n",
              "  'answer': 'Some examples of individual sports mentioned in the document are diving, tennis, gymnastics, and table tennis.'},\n",
              " {'query': 'According to the document, what are the key components involved in constructing a dataset for sports video action recognition?',\n",
              "  'answer': 'The key components involved in constructing a dataset for sports video action recognition, as mentioned in the document, are:'},\n",
              " {'query': 'What is the purpose of the Soccer-ISSIA dataset?',\n",
              "  'answer': 'The Soccer-ISSIA dataset is used for player tracking, detection, and team activity recognition in football.'},\n",
              " {'query': 'What is the purpose of the SpaceJam dataset?',\n",
              "  'answer': 'The purpose of the SpaceJam dataset is to develop skeleton-based action recognition models.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f3cb08",
      "metadata": {
        "id": "63f3cb08"
      },
      "source": [
        "## Manual Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "fcaf622e",
      "metadata": {
        "height": 47,
        "id": "fcaf622e"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "langchain.debug = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a142638",
      "metadata": {
        "height": 30,
        "id": "8a142638"
      },
      "outputs": [],
      "source": [
        "qa.run(examples[3][\"query\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b3d6bef0",
      "metadata": {
        "height": 47,
        "id": "b3d6bef0"
      },
      "outputs": [],
      "source": [
        "# Turn off the debug mode\n",
        "langchain.debug = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5bdbdce",
      "metadata": {
        "id": "d5bdbdce"
      },
      "source": [
        "## LLM assisted evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a4dca05a",
      "metadata": {
        "height": 30,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4dca05a",
        "outputId": "d089c1c7-92f6-4b72-e376-45ac4ef671a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "predictions = qa.apply(reformatted_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6012a3e0",
      "metadata": {
        "height": 30,
        "id": "6012a3e0"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAEvalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "724b1c0b",
      "metadata": {
        "height": 47,
        "id": "724b1c0b"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "eval_chain = QAEvalChain.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(examples), len(predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLhv008INGiZ",
        "outputId": "10c700ad-afc2-4ce3-ccab-0aba6f36bb43"
      },
      "id": "RLhv008INGiZ",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(reformatted_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyXVz4kdh7lZ",
        "outputId": "0e8b3d4c-20bf-4b5d-ae0f-4ceb6ad7c91c"
      },
      "id": "zyXVz4kdh7lZ",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "8b46ae55",
      "metadata": {
        "height": 30,
        "id": "8b46ae55"
      },
      "outputs": [],
      "source": [
        "graded_outputs = eval_chain.evaluate(reformatted_examples, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graded_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzqo4qlqgMna",
        "outputId": "0a696e41-679f-4b17-970f-d4dac62aff46"
      },
      "id": "Mzqo4qlqgMna",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'},\n",
              " {'results': 'CORRECT'}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, eg in enumerate(examples):\n",
        "    if i < len(predictions):\n",
        "        print(f\"Example {i}:\")\n",
        "        print(\"Question: \" + predictions[i]['query'])\n",
        "        print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "        print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "\n",
        "        # Access 'results' key from graded_outputs\n",
        "        predicted_grade = graded_outputs[i].get('results', 'No grade available')\n",
        "        print(\"Predicted Grade: \" + predicted_grade)\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"Example {i}: No prediction available\")\n",
        "        print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lczqSIKjAmf",
        "outputId": "8d01b758-d53f-4dfe-df23-d46ddfc05f93"
      },
      "id": "6lczqSIKjAmf",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "Question: What is the focus of the survey presented in the document?\n",
            "Real Answer: The focus of the survey presented in the document is video action recognition in sports analytics.\n",
            "Predicted Answer: The focus of the survey presented in the document is on action recognition in sports videos. It discusses the challenges and techniques involved in recognizing actions in sports videos, as well as the existing datasets and benchmarks for evaluating performance. The survey also highlights the importance of data collection and annotation in this field and provides references to relevant research papers and resources.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 1:\n",
            "Question: According to the document, what are some examples of individual sports mentioned?\n",
            "Real Answer: Some examples of individual sports mentioned in the document are diving, tennis, gymnastics, and table tennis.\n",
            "Predicted Answer: Some examples of individual sports mentioned in the document are diving, gymnastics, tennis, table tennis, badminton, and figure skating.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 2:\n",
            "Question: According to the document, what are the key components involved in constructing a dataset for sports video action recognition?\n",
            "Real Answer: The key components involved in constructing a dataset for sports video action recognition, as mentioned in the document, are:\n",
            "Predicted Answer: According to the document, the key components involved in constructing a dataset for sports video action recognition are:\n",
            "\n",
            "1. Defining the type of sports and the categories of actions in the specific sport.\n",
            "2. Collecting videos from multiple sources, such as the internet and self-recorded videos.\n",
            "3. Processing the collected videos, including trimming and annotating them.\n",
            "4. Annotations could include trimmed videos with corresponding labels or untrimmed videos with the start and end time of each action and the action category.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 3:\n",
            "Question: What is the purpose of the Soccer-ISSIA dataset?\n",
            "Real Answer: The Soccer-ISSIA dataset is used for player tracking, detection, and team activity recognition in football.\n",
            "Predicted Answer: The purpose of the Soccer-ISSIA dataset is primarily for player tracking, detection, and team activity recognition in soccer videos. It consists of 18,000 high-resolution frames recorded by 6 static cameras. The dataset is processed to extract moving players and annotate their bounding boxes, which are then validated by humans.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 4:\n",
            "Question: What is the purpose of the SpaceJam dataset?\n",
            "Real Answer: The purpose of the SpaceJam dataset is to develop skeleton-based action recognition models.\n",
            "Predicted Answer: The purpose of the SpaceJam dataset is to develop skeleton-based action recognition models for basketball. It collects videos of NBA and Italian championship games from YouTube, along with RGB images and estimated poses of players. The dataset can be used to train models for recognizing various basketball actions.\n",
            "Predicted Grade: CORRECT\n",
            "\n",
            "Example 5: No prediction available\n",
            "\n",
            "Example 6: No prediction available\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "3437cfbe",
      "metadata": {
        "height": 132,
        "id": "3437cfbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "0662f66b-b984-4d08-febb-b1603b7f5d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "Question: What is the focus of the survey presented in the document?\n",
            "Real Answer: The focus of the survey presented in the document is video action recognition in sports analytics.\n",
            "Predicted Answer: The focus of the survey presented in the document is on action recognition in sports videos. It discusses the challenges and techniques involved in recognizing actions in sports videos, as well as the existing datasets and benchmarks for evaluating performance. The survey also highlights the importance of data collection and annotation in this field and provides references to relevant research papers and resources.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-562f1c30d0a5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Real Answer: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Answer: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Grade: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgraded_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(\"Predicted Grade: \" + graded_outputs[i].get('text', 'No grade available'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'text'"
          ]
        }
      ],
      "source": [
        "for i, eg in enumerate(examples):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + predictions[i]['query'])\n",
        "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
        "    #print(\"Predicted Grade: \" + graded_outputs[i].get('text', 'No grade available'))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d95133bb-43b6-441d-9ba5-00ef5609ccc8",
      "metadata": {
        "height": 30,
        "id": "d95133bb-43b6-441d-9ba5-00ef5609ccc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb6dd3c-98a4-4e29-c0b9-c9fd487a011d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'results': 'CORRECT'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "graded_outputs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad0ddd1",
      "metadata": {
        "id": "fad0ddd1"
      },
      "source": [
        "## LangChain evaluation platform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef63bb24",
      "metadata": {
        "id": "ef63bb24"
      },
      "source": [
        "The LangChain evaluation platform, LangChain Plus, can be accessed here https://www.langchain.plus/.  \n",
        "Use the invite code `lang_learners_2023`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b95a2e-3bc7-429a-83af-387239e7f2a1",
      "metadata": {
        "id": "35b95a2e-3bc7-429a-83af-387239e7f2a1"
      },
      "source": [
        "Reminder: Download your notebook to you local computer to save your work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5b2aae",
      "metadata": {
        "height": 30,
        "id": "be5b2aae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319798ba",
      "metadata": {
        "height": 30,
        "id": "319798ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a504ad",
      "metadata": {
        "height": 30,
        "id": "89a504ad"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dedd758b",
      "metadata": {
        "height": 30,
        "id": "dedd758b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36885b20",
      "metadata": {
        "height": 30,
        "id": "36885b20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c6cfb6",
      "metadata": {
        "height": 30,
        "id": "65c6cfb6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad3c7cc",
      "metadata": {
        "height": 30,
        "id": "9ad3c7cc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ac493e",
      "metadata": {
        "height": 30,
        "id": "26ac493e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94cdacd",
      "metadata": {
        "height": 30,
        "id": "f94cdacd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}